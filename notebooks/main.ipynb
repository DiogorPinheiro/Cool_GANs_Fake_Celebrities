{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf2e0f3",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e670c0e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4405d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "\n",
    "os.environ['TFF_CPP_MIN_LOG_LEVEL'] = '2'  # minimize TF annoying messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b0301",
   "metadata": {},
   "source": [
    "Show TF version (2.4.0 and 2.4.1 are almost the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd11618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "GPUs used: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(f\"GPUs used: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b7db9",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e2537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline_load(dataset_name, **kwargs):\n",
    "    \"\"\" Create optimized data loading pipeline for training.\n",
    "    Adapted from: https://cs230.stanford.edu/blog/datapipeline/,\n",
    "    https://www.tensorflow.org/tutorials/load_data/images#standardize_the_data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : str\n",
    "        Name of dataset, either mnist or faces.\n",
    "    kwargs : dict\n",
    "        Must include resize_to for both nist and faces. Must include data_directory for faces.\n",
    "        If resize_to is None, the images keep their original shapes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : tf.data.Dataset\n",
    "        Dataset object.\n",
    "    dataset_size : int\n",
    "        Number of instances in dataset. For shuffling later.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Dataset is not pre-processed, not shuffled, not batched and not pre-fetched yet.\n",
    "    \"\"\"\n",
    "    resize_to = kwargs[\"resize_to\"]\n",
    "    \n",
    "    if dataset_name == \"mnist\":\n",
    "        dataset, dataset_size, img_original_shape = data_pipeline_load_mnist()\n",
    "    elif dataset_name == \"faces\":\n",
    "        data_directory= kwargs[\"data_directory\"]\n",
    "        dataset, dataset_size, img_original_shape = data_pipeline_load_faces(data_directory)\n",
    "    \n",
    "    resize_to = img_original_shape if resize_to is None else resize_to\n",
    "    \n",
    "    dataset = data_pipeline_pre_process(dataset, resize_to)\n",
    "    \n",
    "    print(f\"loaded and pre-processed: {dataset_name}, size: {dataset_size}\")\n",
    "    \n",
    "    return dataset, dataset_size\n",
    "\n",
    "def data_pipeline_pre_process(dataset, resize_to):\n",
    "    \"\"\" Create optimized data pre-processing pipeline for training.\n",
    "    Adapted from: https://cs230.stanford.edu/blog/datapipeline/,\n",
    "    https://www.tensorflow.org/tutorials/load_data/images#standardize_the_data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : str\n",
    "        Name of dataset, either mnist or faces.\n",
    "    resize_to : tuple\n",
    "        Resize to (height, width).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : tf.data.Dataset\n",
    "        Dataset object.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Dataset is pre-processed - standardized to -1.0 - 1.0 and resized.\n",
    "    But not shuffled, not batched and not pre-fetched yet.\n",
    "    \"\"\"\n",
    "    normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1)\n",
    "    \n",
    "    # automatically finds good allocation of CPU budget\n",
    "    dataset = dataset.map(normalization_layer, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    # automatically finds good allocation of CPU budget\n",
    "    dataset = dataset.map(lambda im: resize_image(im, resize_to), \n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def data_pipeline_pre_train(dataset, dataset_size, batch_size):\n",
    "    \"\"\" Create optimized pre-training data pipeline. Shuffle, batch, and pre-fetch dataset.\n",
    "    Adapted from: https://cs230.stanford.edu/blog/datapipeline/,\n",
    "    https://www.tensorflow.org/tutorials/load_data/images#standardize_the_data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : tf.data.Dataset\n",
    "        Dataset object.\n",
    "    dataset_size : int\n",
    "        Number of instances in dataset. For shuffling.\n",
    "    batch_size : int \n",
    "        Batch size.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : tensorflow.data.Dataset\n",
    "        Dataset object.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Dataset is now pre-processed, shuffled, batched and pre-fetched yet.\n",
    "    buffer_size (dataset_size) >= batch_size to get an uniform shuffle, right? idk\n",
    "    \"\"\"\n",
    "    dataset = dataset.shuffle(dataset_size).batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def resize_image(image, resize_to):\n",
    "    \"\"\" Resize image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : tensorflow.Tensor\n",
    "        Image object.\n",
    "    resize_to : tuple\n",
    "        Resize to (height, width).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    resized_image : tensorflow.Tensor\n",
    "        Resize image object.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    None\n",
    "    \"\"\"\n",
    "    resized_image = tf.image.resize(image, [resize_to[0], resize_to[1]])\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def data_pipeline_load_mnist():\n",
    "    \"\"\" MNIST loading pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : tf.data.Dataset\n",
    "        Dataset object.\n",
    "    dataset_size : int\n",
    "        Number of instances in dataset. For shuffling.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    None\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (_, _) = mnist.load_data()\n",
    "    \n",
    "    img_original_shape = (28, 28)\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    \n",
    "    # shuffle here since more difficult later\n",
    "    dataset_size = len(x_train)\n",
    "    \n",
    "    return dataset, dataset_size, img_original_shape\n",
    "\n",
    "def get_faces_paths(data_directory):\n",
    "    \"\"\" Get the paths to the faces images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_directory : str\n",
    "        Path to the faces images.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    paths_faces : list\n",
    "        List of paths to all images.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    None\n",
    "    \"\"\"\n",
    "    paths_faces = []\n",
    "        \n",
    "    for idx, filename in enumerate(os.listdir(data_directory)):\n",
    "        paths_face = os.path.join(data_directory, filename)\n",
    "        paths_faces.append(paths_face)\n",
    "    \n",
    "    return paths_faces\n",
    "\n",
    "def parse_function(filename):\n",
    "    \"\"\" Parse a filename and load the corresponding image. Used for faces.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the faces image.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image : tensorflow.Tensor\n",
    "        Image object.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    None\n",
    "    \"\"\"\n",
    "    image_string = tf.io.read_file(filename)\n",
    "\n",
    "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    # automatically finds good allocation of CPU budget\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    #This will convert to float values in [0, 1]\n",
    "    #image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    return image\n",
    "\n",
    "def data_pipeline_load_faces(data_directory):\n",
    "    \"\"\" Faces loading pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_directory : str\n",
    "        Path to the faces images.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : tf.data.Dataset\n",
    "        Dataset object.\n",
    "    dataset_size : int\n",
    "        Number of instances in dataset. For shuffling.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    None\n",
    "    \"\"\"\n",
    "    filenames = get_faces_paths(data_directory)\n",
    "    dataset_size = len(filenames)\n",
    "    \n",
    "    img = cv2.imread(filenames[0])\n",
    "    img_original_shape = img.shape[:2]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    #dataset = dataset.shuffle(len(filenames))\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset, dataset_size, img_original_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9358414",
   "metadata": {},
   "source": [
    "### Check datapipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c570f12",
   "metadata": {},
   "source": [
    "for faces. images should be resized to ```resize_to``` (if None, images keep their original shapes) and min-max should be -1.0 - 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fff320a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ndata_directory = os.path.join(\"../\", \"data\", \"archive\", \"img_align_celeba\", \"img_align_celeba\")\\n# original shape = (218, 178, 3)\\nresize_to = (32, 32)\\n#resize_to = None\\nkwargs = {\"data_directory\": data_directory, \"resize_to\": resize_to}\\ndataset_name = \"faces\"\\nbatch_size = 128\\n\\ndataset, dataset_size = data_pipeline_load(dataset_name, **kwargs)\\n\\nprint(dataset)\\n\\n# warning: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\\n# eliminate by inverse pre-processing steps during visualization\\nfor image in dataset.take(2):\\n    plt.imshow(image)\\n    plt.show()\\n    print(image.shape)\\n    print(tf.reduce_max(image))\\n    print(tf.reduce_min(image))\\n    \\ndataset = data_pipeline_pre_train(dataset, dataset_size, batch_size)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "data_directory = os.path.join(\"../\", \"data\", \"archive\", \"img_align_celeba\", \"img_align_celeba\")\n",
    "# original shape = (218, 178, 3)\n",
    "resize_to = (32, 32)\n",
    "#resize_to = None\n",
    "kwargs = {\"data_directory\": data_directory, \"resize_to\": resize_to}\n",
    "dataset_name = \"faces\"\n",
    "batch_size = 128\n",
    "\n",
    "dataset, dataset_size = data_pipeline_load(dataset_name, **kwargs)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# warning: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "# eliminate by inverse pre-processing steps during visualization\n",
    "for image in dataset.take(2):\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(image.shape)\n",
    "    print(tf.reduce_max(image))\n",
    "    print(tf.reduce_min(image))\n",
    "    \n",
    "dataset = data_pipeline_pre_train(dataset, dataset_size, batch_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15b090",
   "metadata": {},
   "source": [
    "for mnist. images should be resized to ```resize_to``` (if None, images keep their original shapes) and min-max should be -1.0 - 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93dd9c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and pre-processed: mnist, size: 60000\n",
      "<ParallelMapDataset shapes: (28, 28, 1), types: tf.float32>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# original shape = (28, 28, 1)\n",
    "resize_to = (32, 32)\n",
    "resize_to = None\n",
    "kwargs = {\"data_directory\": None, \"resize_to\": resize_to}\n",
    "dataset_name = \"mnist\"\n",
    "batch_size = 128\n",
    "\n",
    "dataset, dataset_size = data_pipeline_load(dataset_name, **kwargs)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "for image in dataset.take(2):\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(image.shape)\n",
    "    print(tf.reduce_max(image))\n",
    "    print(tf.reduce_min(image))\n",
    "    \n",
    "dataset = data_pipeline_pre_train(dataset, dataset_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982b44f",
   "metadata": {},
   "source": [
    "## DCGAN model for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e715c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_out_size_same(size, stride):\n",
    "    \"\"\" Faces loading pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int\n",
    "        Size of dimension (height or width) of layer n.\n",
    "    stride : int\n",
    "        Stride size in that dimension of layer n.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The size of the dimension (height or width) in layer n-1 to obtain same convoltuion\n",
    "        from layer n-1 to n.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Source:\n",
    "    https://github.com/carpedm20/DCGAN-tensorflow/blob/62c9a2a7f74505cad30858bf40307c93e5bd9293/model.py#L14\n",
    "    \"\"\"\n",
    "    return int(math.ceil(float(size) / float(stride)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c0ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(keras.Model):\n",
    "    def __init__(self, input_shape, name, **cfg):\n",
    "        super(DCGAN, self).__init__(name=name)\n",
    "        #self.make_generator(input_shape)\n",
    "        self.cfg = cfg\n",
    "        self.gen_cfg = cfg[\"generator\"]\n",
    "        #self.disc_cfg = cfg[\"discriminator\"]\n",
    "    \n",
    "    def complement_gen_cfg(self, gen_cfg, output_shape):\n",
    "        \"\"\" Build config for generator. Config is yaml file. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gen_cfg : dict\n",
    "            Generator config dict from yaml file.\n",
    "        output_shape : tuple\n",
    "            Height, width, channels of output feature map.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            gen_cfg but complemented by feature map shapes (heigh, width), most importantly\n",
    "            with the project-reshape z shape.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        E.g.: the architecture in the paper (gen_cfg):\n",
    "        \n",
    "        generator:\n",
    "          pr:\n",
    "            filters: 1024\n",
    "          h1:\n",
    "            filters: 512\n",
    "            strides: [2,2]\n",
    "            kernel_size: [5,5]\n",
    "          h2:\n",
    "            filters: 256\n",
    "            strides: [2,2]\n",
    "            kernel_size: [5,5]\n",
    "          h3:\n",
    "            filters: 128\n",
    "            strides: [2,2]\n",
    "            kernel_size: [5,5]\n",
    "          h4:\n",
    "            filters: 3\n",
    "            strides: [2,2]\n",
    "            kernel_size: [5,5]\n",
    "        \n",
    "        and output_shape is (64,64,3).\n",
    "        Then, \n",
    "        \n",
    "        s_h, s_w, s_c = output_shape = (64,64,3)\n",
    "        \n",
    "        s_c4 = 3 -> generator[\"h4\"][\"filters\"]\n",
    "        s_c3 = 512 -> generator[\"h3\"][\"filters\"]\n",
    "        s_c2 = 256 -> generator[\"h2\"][\"filters\"]\n",
    "        s_c1 = 512 -> generator[\"h1\"][\"filters\"]\n",
    "        s_c_pr = 1024 -> generator[\"pr\"][\"filters\"]\n",
    "        \n",
    "        s_s4 = (2, 2) -> generator[\"h4\"][\"strides\"]\n",
    "        s_s3 = (2, 2) -> generator[\"h3\"][\"strides\"]\n",
    "        s_s2 = (2, 2) -> generator[\"h2\"][\"strides\"]\n",
    "        s_s1 = (2, 2) -> generator[\"h1\"][\"strides\"]\n",
    "        \n",
    "        s_f4 = (5,5) -> generator[\"h4\"][\"kernel_size\"]\n",
    "        s_f3 = (5,5) -> generator[\"h3\"][\"kernel_size\"]\n",
    "        s_f2 = (5,5) -> generator[\"h2\"][\"kernel_size\"]\n",
    "        s_f1 = (5,5) -> generator[\"h1\"][\"kernel_size\"]\n",
    "        \n",
    "        s_h3, s_w3 = conv_out_size_same(s_h, s_s4[0]), conv_out_size_same(s_w, s_s4[1]) = (32,32)\n",
    "        s_h2, s_w2 = conv_out_size_same(s_h3, s_s3[0]), conv_out_size_same(s_w3, s_s3[1]) = (16,16)\n",
    "        s_h1, s_w1 = conv_out_size_same(s_h2, s_s2[0]), conv_out_size_same(s_w2, s_s2[1]) = (8,8)\n",
    "        s_h_pr, s_w_pr = conv_out_size_same(s_h1, s_s1[0]), conv_out_size_same(s_w1, s_s1[1]) = (4,4)\n",
    "        \n",
    "        where e.g.: s_h3, s_w3 is the feature map shape (h,w) in the 3rd upsampling conv2\n",
    "        and s_h_pr, s_w_pr is the shape (h,w) of the project-reshape on z.\n",
    "        Each hx layer is an upsampling conv2d layer.\n",
    "        \n",
    "        Note that upsampling conv2d is always same convolution.\n",
    "        The whole point of this function is to obtain the feature map shapes and the project-reshape z\n",
    "        shape from the output_shape, the number of filters (filters), the stride size (strides),\n",
    "        and the kernel sizes.\n",
    "        \"\"\"\n",
    "        # Get output shape.\n",
    "        s_h, s_w, s_c = output_shape\n",
    "        \n",
    "        # Get the layer keys.\n",
    "        keys = list(gen_cfg.keys())\n",
    "        keys_reversed = list(reversed(keys))\n",
    "        \n",
    "        # Set temp variables.\n",
    "        s_hx = s_h\n",
    "        s_wx = s_w\n",
    "        \n",
    "        # Reverse iterate through layers.\n",
    "        for idx, k in enumerate(keys_reversed):\n",
    "            # if project-reshape z (layer no. 0), then set the last layer \n",
    "            # (layer before pr in reverse, last list element) feature map shape\n",
    "            if k == \"pr\":\n",
    "                gen_cfg[keys_reversed[0]][\"s_h\"] = s_h\n",
    "                gen_cfg[keys_reversed[0]][\"s_w\"] = s_w\n",
    "                # exit loop\n",
    "                continue\n",
    "            \n",
    "            # get stride size\n",
    "            s_sx = gen_cfg[k][\"strides\"]\n",
    "            \n",
    "            # get feature map shape\n",
    "            s_hx, s_wx = \\\n",
    "                conv_out_size_same(s_hx, s_sx[0]), conv_out_size_same(s_wx, s_sx[1])\n",
    "            \n",
    "            # set the feature map shape in the layer before based on the current layer\n",
    "            gen_cfg[keys_reversed[idx+1]][\"s_h\"] = s_hx\n",
    "            gen_cfg[keys_reversed[idx+1]][\"s_w\"] = s_wx\n",
    "            \n",
    "        return deepcopy(gen_cfg) \n",
    "    \n",
    "    def make_generator(self, input_shape, output_shape):\n",
    "        \"\"\" Make the generator model. Note that depending on the dataset, the feature map shapes,\n",
    "        kernel sizes, and channels will change. Encapsulated config in yaml file to automatically\n",
    "        do this.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Input image shape, height, width, channels.\n",
    "        output_shape : tuple\n",
    "            Output image shape, height, width, channels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        keras.Model\n",
    "            The generator model.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        # complement the generator config with the feature map shapes, and\n",
    "        # most importantly with the proejct-reshape z shape.\n",
    "        gen_cfg = self.complement_gen_cfg(self.gen_cfg, output_shape)\n",
    "        \n",
    "        # take care of input z (!!)\n",
    "        # Functional API for more flexibility\n",
    "        inputs = keras.Input(shape=input_shape, name=\"generator_input\")\n",
    "        \n",
    "        # NOTE: Number of filters should be adapted to each dataset <- FIND WAY TO DO IT AUTOMATICALLY\n",
    "        # idea: define build_mnist_generator() and use that in if else\n",
    "        # project `z` and reshape\n",
    "        s_h_pr = gen_cfg[\"pr\"][\"s_h\"]\n",
    "        s_w_pr = gen_cfg[\"pr\"][\"s_w\"]\n",
    "        s_c_pr = gen_cfg[\"pr\"][\"filters\"]\n",
    "        z_proj = layers.Dense(s_h_pr*s_w_pr*s_c_pr, use_bias=False, name=\"g_h0_lin\")(inputs)\n",
    "        h = layers.Reshape((s_h_pr, s_w_pr, s_c_pr), name=\"g_h0\")(z_proj)\n",
    "        h = layers.BatchNormalization(name=\"g_h0_bn\")(h)\n",
    "        h = layers.LeakyReLU(name=\"g_h0_a\")(h)\n",
    "        \n",
    "        # Note: None is the batch size\n",
    "        assert tuple(h.shape) == (None, s_h_pr, s_w_pr, s_c_pr)\n",
    "        \n",
    "        # get layer keys\n",
    "        keys = list(gen_cfg.keys())\n",
    "        \n",
    "        # iterate through the layers from the beginning\n",
    "        # skip project-reshape z, and deal with last upsample conv2d later\n",
    "        for k in keys[1:-1]:\n",
    "            # upsample conv2d layer\n",
    "            s_c = gen_cfg[k][\"filters\"]\n",
    "            s_s = gen_cfg[k][\"strides\"]\n",
    "            s_f = gen_cfg[k][\"kernel_size\"]\n",
    "            \n",
    "            h = layers.Conv2DTranspose(filters=s_c, \n",
    "                                       kernel_size=s_f, \n",
    "                                       strides=s_s, \n",
    "                                       padding='same', \n",
    "                                       use_bias=False, \n",
    "                                       name=f\"g_{k}\")(h)\n",
    "\n",
    "            h = layers.BatchNormalization(name=f\"g_{k}_bn\")(h)\n",
    "            h = layers.LeakyReLU(name=f\"g_{k}_a\")(h)\n",
    "            \n",
    "            s_h = gen_cfg[k][\"s_h\"]\n",
    "            s_w = gen_cfg[k][\"s_w\"]\n",
    "            \n",
    "            assert tuple(h.shape) == (None, s_h, s_w, s_c)\n",
    "\n",
    "        # upsample conv2d last layer\n",
    "        k = keys[-1]\n",
    "        s_c = gen_cfg[k][\"filters\"]\n",
    "        s_s = gen_cfg[k][\"strides\"]\n",
    "        s_f = gen_cfg[k][\"kernel_size\"]\n",
    "        \n",
    "        h = layers.Conv2DTranspose(filters=s_c, \n",
    "                                   kernel_size=s_f, \n",
    "                                   strides=s_s, \n",
    "                                   padding='same', \n",
    "                                   use_bias=False, \n",
    "                                   activation='tanh',\n",
    "                                   name=f\"g_{k}\")(h)\n",
    "        \n",
    "        s_h = gen_cfg[k][\"s_h\"]\n",
    "        s_w = gen_cfg[k][\"s_w\"]\n",
    "        \n",
    "        assert tuple(h.shape) == (None, s_h, s_w, s_c)\n",
    "        \n",
    "        self.generator = tf.keras.Model(inputs=inputs, outputs=h, name=\"generator\")\n",
    "        \n",
    "        return self.generator\n",
    "\n",
    "    def make_discriminator(self, output_shape):\n",
    "        \"\"\" Make the discriminator model. Model is fixed (at least for now).\n",
    "        Can go down from arbitrary output_shape to sigmoid prediciton.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        output_shape : tuple\n",
    "            Output image shape, height, width, channels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        keras.Model\n",
    "            The discriminator model.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        None\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        None\n",
    "        \"\"\"\n",
    "        outputs = keras.Input(shape=output_shape, name=\"discriminator_input\")\n",
    "        \n",
    "        h = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',name=\"d_h0\")(outputs)\n",
    "        h = layers.LeakyReLU()(h)\n",
    "        h = layers.Dropout(0.3)(h)\n",
    "\n",
    "        h = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', name=\"d_h1\")(h)\n",
    "        h = layers.LeakyReLU()(h)\n",
    "        h = layers.Dropout(0.3)(h)\n",
    "\n",
    "        h = layers.Flatten(name=\"d_h2_lin\")(h)\n",
    "        h = layers.Dense(1)(h)\n",
    "        \n",
    "        self.discriminator = tf.keras.Model(inputs=outputs, outputs=h, name=\"discriminator\")\n",
    "\n",
    "        return self.discriminator\n",
    "    \n",
    "    def evaluate_loss(self,real_images,fake_images,type):\n",
    "        '''\n",
    "                Generator -> Goal is to generate the \"realest\" fake images, so the loss is only measured taking those in consideration.\n",
    "\n",
    "                Discriminator -> We want the output to be 1 for real images and 0 for fake images. \n",
    "                Here, the total loss will be aggregated loss for real and fake images (which is obvious because we're trying \n",
    "                to see how these compare)\n",
    "\n",
    "        '''\n",
    "        f_loss = keras.losses.binary_crossentropy(from_logits=True)\n",
    "        if type == 'generator':\n",
    "            return f_loss(tf.ones_like(fake_images),fake_images).numpy()    # transform fake images array into an array of ones\n",
    "        elif type == 'discriminator':\n",
    "            return f_loss(tf.ones_like(real_images),real_images).numpy() + f_loss(tf.zeros_like(fake_images),fake_images).numpy()\n",
    "\n",
    "    def optimize(learning_rate=0.0002,beta1=0.5):\n",
    "        '''\n",
    "            According to the original DCGAN paper(*): \n",
    "                1) Learning rate - \" While previous GAN work has used momentum to accelerate training, \n",
    "            we used the Adam optimizer (Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning \n",
    "            rate of 0.001, to be too high, using 0.0002 instead.\"\n",
    "\n",
    "                2) Momentum (beta1) - \"Additionally, we found leaving the momentum term β1 at the suggested value of 0.9 resulted\n",
    "                 in training oscillation and instability while reducing it to 0.5 helped stabilize training.\n",
    "\n",
    "\n",
    "            (*) https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "\n",
    "            Output:\n",
    "                Optimizer for both generator and discriminator model\n",
    "        '''\n",
    "        return keras.optimizers.Adam(learning_rate,beta_1=beta1), keras.optimizers.Adam(learning_rate,beta_1=beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0665b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "g_h0_lin (Dense)             (None, 12544)             1254400   \n",
      "_________________________________________________________________\n",
      "g_h0 (Reshape)               (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "g_h0_bn (BatchNormalization) (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "g_h0_a (LeakyReLU)           (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "g_h1 (Conv2DTranspose)       (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "g_h1_bn (BatchNormalization) (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "g_h1_a (LeakyReLU)           (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "g_h2 (Conv2DTranspose)       (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "g_h2_bn (BatchNormalization) (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "g_h2_a (LeakyReLU)           (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "g_h3 (Conv2DTranspose)       (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,281,792\n",
      "Trainable params: 2,280,896\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "No algorithm worked! [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-647eee43ea45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mgenerated_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \"\"\"\n\u001b[1;32m--> 424\u001b[1;33m     return self._run_internal_graph(\n\u001b[0m\u001b[0;32m    425\u001b[0m         inputs, training=training, mask=mask)\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[0moutput_shape_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m     outputs = backend.conv2d_transpose(\n\u001b[0m\u001b[0;32m   1309\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[1;34m(x, kernel, output_shape, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   5349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5350\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdilation_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5351\u001b[1;33m     x = nn.conv2d_transpose(x, kernel, output_shape, strides,\n\u001b[0m\u001b[0;32m   5352\u001b[0m                             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5353\u001b[0m                             data_format=tf_data_format)\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[1;34m(value, filter, output_shape, strides, padding, data_format, name, input, filters, dilations)\u001b[0m\n\u001b[0;32m   2485\u001b[0m   with ops.name_scope(name, \"conv2d_transpose\",\n\u001b[0;32m   2486\u001b[0m                       [value, filter, output_shape]) as name:\n\u001b[1;32m-> 2487\u001b[1;33m     return conv2d_transpose_v2(\n\u001b[0m\u001b[0;32m   2488\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2489\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose_v2\u001b[1;34m(input, filters, output_shape, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2569\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2571\u001b[1;33m     return gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[0;32m   2572\u001b[0m         \u001b[0minput_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2573\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: No algorithm worked! [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "with open(\"config_tf_example.yml\", 'r') as stream:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "input_shape = (100,)\n",
    "output_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "dcgan = DCGAN(input_shape=input_shape, name=\"my_dcgan\", **cfg)\n",
    "\n",
    "generator = dcgan.make_generator(input_shape, output_shape)\n",
    "\n",
    "print(generator.summary())\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=True)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "\n",
    "discriminator = dcgan.make_discriminator(output_shape)\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)\n",
    "\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af3316b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "g_h0_lin (Dense)             (None, 16384)             1638400   \n",
      "_________________________________________________________________\n",
      "g_h0 (Reshape)               (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "g_h0_bn (BatchNormalization) (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "g_h0_a (LeakyReLU)           (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "g_h1 (Conv2DTranspose)       (None, 8, 8, 512)         13107200  \n",
      "_________________________________________________________________\n",
      "g_h1_bn (BatchNormalization) (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "g_h1_a (LeakyReLU)           (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "g_h2 (Conv2DTranspose)       (None, 16, 16, 256)       3276800   \n",
      "_________________________________________________________________\n",
      "g_h2_bn (BatchNormalization) (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "g_h2_a (LeakyReLU)           (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "g_h3 (Conv2DTranspose)       (None, 32, 32, 128)       819200    \n",
      "_________________________________________________________________\n",
      "g_h3_bn (BatchNormalization) (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "g_h3_a (LeakyReLU)           (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "g_h4 (Conv2DTranspose)       (None, 64, 64, 3)         9600      \n",
      "=================================================================\n",
      "Total params: 18,858,880\n",
      "Trainable params: 18,855,040\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "No algorithm worked! [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-da4bfd9e9524>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mgenerated_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \"\"\"\n\u001b[1;32m--> 424\u001b[1;33m     return self._run_internal_graph(\n\u001b[0m\u001b[0;32m    425\u001b[0m         inputs, training=training, mask=mask)\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[0moutput_shape_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m     outputs = backend.conv2d_transpose(\n\u001b[0m\u001b[0;32m   1309\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[1;34m(x, kernel, output_shape, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   5349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5350\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdilation_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5351\u001b[1;33m     x = nn.conv2d_transpose(x, kernel, output_shape, strides,\n\u001b[0m\u001b[0;32m   5352\u001b[0m                             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5353\u001b[0m                             data_format=tf_data_format)\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[1;34m(value, filter, output_shape, strides, padding, data_format, name, input, filters, dilations)\u001b[0m\n\u001b[0;32m   2485\u001b[0m   with ops.name_scope(name, \"conv2d_transpose\",\n\u001b[0;32m   2486\u001b[0m                       [value, filter, output_shape]) as name:\n\u001b[1;32m-> 2487\u001b[1;33m     return conv2d_transpose_v2(\n\u001b[0m\u001b[0;32m   2488\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2489\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose_v2\u001b[1;34m(input, filters, output_shape, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2569\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2571\u001b[1;33m     return gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[0;32m   2572\u001b[0m         \u001b[0minput_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2573\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: No algorithm worked! [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "with open(\"config_paper.yml\", 'r') as stream:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "input_shape = (100,)\n",
    "output_shape = (64, 64, 3)\n",
    "\n",
    "\n",
    "dcgan = DCGAN(input_shape=input_shape, name=\"my_dcgan\", **cfg)\n",
    "\n",
    "generator = dcgan.make_generator(input_shape, output_shape)\n",
    "\n",
    "print(generator.summary())\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "\n",
    "discriminator = dcgan.make_discriminator(output_shape)\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)\n",
    "\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cbd173",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yaml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1bbca4f936ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYAMLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yaml' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1bbca4f936ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYAMLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yaml' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"config_custom.yml\", 'r') as stream:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "input_shape = (100,)\n",
    "output_shape = (128, 128, 3)\n",
    "\n",
    "\n",
    "dcgan = DCGAN(input_shape=input_shape, name=\"my_dcgan\", **cfg)\n",
    "\n",
    "generator = dcgan.make_generator(input_shape, output_shape)\n",
    "\n",
    "print(generator.summary())\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "\n",
    "discriminator = dcgan.make_discriminator(output_shape)\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)\n",
    "\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d93715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e8ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71269d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "f714f05e6ffb5ded92074cd2d65d63a36667b153e7558a65abc88f4ed2fc286e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}